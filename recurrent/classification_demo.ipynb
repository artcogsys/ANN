{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import chainer\n",
    "import modelzoo as m\n",
    "import numpy\n",
    "import random\n",
    "import recurrent_neural_network as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Classification example ###\n",
    "\n",
    "# First, we define the data. For the sake of simplicity, we define the inputs as randomly\n",
    "# generated timeseries. That is, each input time point comprises two random numbers between\n",
    "# zero and one. Then, we define the targets as a function of the inputs. That is, each output\n",
    "# time point is zero if the sum of the previous input time point is less than one or one if\n",
    "# the sum of the previous input time point is greater than one or one.\n",
    "\n",
    "X = {}\n",
    "T = {}\n",
    "\n",
    "for phase in ['training', 'validation']:\n",
    "    X[phase] = [[random.random(), random.random()] for _ in xrange(1000)]\n",
    "    T[phase] = [0] + [0 if sum(i) < 1.0 else 1 for i in X[phase]][:-1]\n",
    "    X[phase] = numpy.array(X[phase], 'float32')\n",
    "    T[phase] = numpy.array(T[phase], 'int32')\n",
    "\n",
    "# Next we define the model:\n",
    "compute_accuracy = True # optional for classification, must be false for regression.\n",
    "experiment = 'classification' # this is just a name for the experiment\n",
    "id = None # gpu id; if None, cpu is used\n",
    "lossfun = chainer.functions.softmax_cross_entropy # chainer lossfun. SCE for classification.\n",
    "optimizer = chainer.optimizers.Adam() # chainer optimizer\n",
    "predictor = m.RNN(2, 10, 2) # a predictor; either from  model_zoo or custom defined\n",
    "\n",
    "RNN = r.RecurrentNeuralNetwork(compute_accuracy, experiment, id, lossfun, optimizer, predictor)\n",
    "\n",
    "# And the optimization parameters:\n",
    "cutoff = 10 # cutoff steps for truncated backprop.\n",
    "epochs = 100 # number of epochs to train (# passes over the entire data)\n",
    "seqs = 32 # number of mini batches\n",
    "callback = r.callback # an optional function called after each epoch (see example in script)\n",
    "rate_lasso = None # regularization coefficient for L1 term\n",
    "rate_weight_decay = 1e-5  # regularization coefficient for L2 term\n",
    "threshold = 5 # threshold for gradient clipping\n",
    "\n",
    "# Finally we run the optimization\n",
    "# Note: to use a model after optimization, the predict method should be used; train and test\n",
    "# methods are for internal use only.\n",
    "RNN.optimize(T, X, cutoff, epochs, seqs, callback, rate_lasso, rate_weight_decay, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
